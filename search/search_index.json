{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Footy RL","text":""},{"location":"#motivation","title":"Motivation","text":"<p>The rise of LLMs has completely taken over career, but I've grown miss the world of classical ML and RL. I wanted to refresh my memory on some of the fundamentals and take on the challenge of designing a RL environment from scratch. I also wanted to practice greenfield project development with AI assisted tools, seeing how much they could accelerate my pace of development. </p> <p>I found this to be a rewarding project with fun results. It supported my belief that experienced developers can use AI coding tools as a force multiplier rather than atrophy-inducing crutch. </p>"},{"location":"#project-summary","title":"Project Summary","text":"<p>This project tackles a multi-agent adversarial reinforcement learning problem by simulating a competitive soccer environment. In this setting, two teams of agents engage in a dynamic game where each player controls movement and kicking actions with the goal of scoring and defending, all while navigating continuous state and action spaces. The agents learn through self-play, contending with both cooperative strategies among teammates and adversarial tactics from opponents. </p> <p>The gameplay environment was build specially for this project and is completely novel. It features simple ball physics, boundary constraints and game rules. It takes place in continuous space over discrete time intervals. </p> <p>A reward signal shapes the behavior of agents by rewarding goal scoring. It optionally can also reward proximity to the ball and posession (kicks). </p>"},{"location":"blog/","title":"Experiments","text":""},{"location":"blog/2025/07/29/einsum-optimization-and-deployment/","title":"Einsum Optimization and Deployment","text":"<p>Returning to this project after three months with the help of claude code. My goal is to demonstrate true cooperative multi-agent behavior when trained using adversarial self-play. </p> <p>But before this, I wanted to make sure that I was really getting great performance during training. Recall that back in April, I had tensorized the game environment so that the training loop could run without passing data between CPU and GPU. I was able to simulate approximately 256 games per second (batch size of 512 running every 2 seconds). </p> <p>I knew I still had plenty of room to optimize, and the easiest way to do that is to look for places to use Einsum notation, which is often a more optimized way of expressing operations that I had previously expressed with clumsy chains of sums, products and reductions. </p> <p>Einsum notation was invented by Einstein more than a hundred years ago to help him represent the higher order matrix operations that he was using in his calculations. Fast forward to the present day and it can help us reduce intermediate tensor memory allocations, and leverage highly optimized einsum kernels to squeeze the last bit of performance from the compute graph optimizer. </p> <p>I also wanted to streamline my deployment process so that I could run experiments without having it take up too much mental bandwidth. Inspired by my workflow in TinyDiffusionModels, I created a lightweight deployment harness with special instructions so that Claude Code could operate it at my behest. I've been finding this much more engoyable to operate than my old ML Ops tangles of second order configuratios. </p> <p>Results were very positive, showing a 2x speedup in batch iteration speed and very quick convergence.</p>"},{"location":"blog/2025/04/14/v1-development-and-experimentation/","title":"V1 Development and Experimentation","text":""},{"location":"blog/2025/04/14/v1-development-and-experimentation/#functional-core","title":"Functional Core","text":"<p>My initial goal was to set up the environment with a pythonic functional core. I created a <code>GameState</code> object represented all the data at a given point in time, as well as <code>PlayerAction</code> objects. A function of the game state and player actions (from all players on both teams) determined the <code>GameState</code> at the next timestep. </p> <p>Since the code was in pure python/pydantic and functional, it was easy to test rigorously. I also created a visualizer function that converted an array of game states to an MP4 video. All of my tests wrote out videos so that I could eyeball the logic as needed. </p>"},{"location":"blog/2025/04/14/v1-development-and-experimentation/#environment","title":"Environment","text":"<p>Inspired by OpenAI's <code>Gymnasium</code>, I created an environment with a <code>.step()</code> method that followed the standard interface: it takes in a set of actions for the step, and produces a set of observations for the next timestep, as well as a reward score and basic information about the environment state. </p> <p>The actions and observations are both flat numpy <code>ndarray</code> types. Even though I was aiming to develop an adversarial multi-agent game, I didnt have any special handling for the two teams in the interface. The actions of each team were simply concatenated together, and the observations were, or course, identical. I would have to handle all the multi-agent logic in my training script later on. </p> <p>Note that, since I had defined all my game logic in the <code>GameState</code> functional data model, the environment itself was just a thin layer that mediated between <code>ndarray</code> types (that my neural networks would work with) and the native pydantic objects of the data model. </p> <p>I'll talk about the reward (returned by the <code>.step()</code> method alongside the observation) more later on, but I started with a simple score-based reward. </p>"},{"location":"blog/2025/04/14/v1-development-and-experimentation/#policy-network","title":"Policy Network","text":"<p>I created the simplest possible policy network that would support the <code>REINFORCE</code> algorithm. This is simply a neural network that outputs a <code>mu</code> and <code>sigma</code> (each of shape <code>output_size</code>), which will be the parameters of a Normal distribution used for REINFOCE monte carlo sampling. </p>"},{"location":"blog/2025/04/14/v1-development-and-experimentation/#training-loop","title":"Training Loop","text":"<p>To begin training, I implemented a basic self-play loop using the REINFORCE algorithm. Two policy networks\u2014one for each team\u2014were initialized independently. On each timestep  both teams produced actions from their policy networks based on the current observation. These actions were sampled from Normal distributions parameterized by the network outputs (<code>mu</code>, <code>sigma</code>), and log-probabilities (log-likelihoods in Bayesian terms) were stored for each timestep to calculate the eventual policy gradient loss.</p>"},{"location":"blog/2025/04/14/v1-development-and-experimentation/#discounted-reward-computation","title":"Discounted Reward Computation","text":"<p>The environment produces a reward value every time step, but we dont use this directly for gradient descent. </p> <p>Following the REINFORCE approach, I computed discounted rewards at the end of each episode and normalized them, then applied the REINFORCE loss to update each team\u2019s policy network via gradient descent. Discounted rewards set the reward at any given timestep to the sum of future rewards, discounted by a decay factor, <code>gamma</code> (typically .99). The intuition here is that an action taken at time <code>t</code> might cause or contribute to a reward at time <code>t+N</code>, and so should be reinforced. </p> <p>The rewards were handled in a simple zero-sum format: team A received the reward from the environment (after discounted summation and normalization as above), while team B received its negation. </p>"},{"location":"blog/2025/04/14/v1-development-and-experimentation/#compute","title":"Compute","text":"<p>The training loop with 5k iterations of a batch of 64 games could run in about an hour on my macbook's CPU. </p> <p>Why not use GPU hardware? Well my environment relies on a functional data model in raw python for the game simulation. In other words, whether the model is in the GPU or CPU, something will happen on the CPU every timestep. Putting the model on the GPU would likely make things slower due to data transfer / synchronization time. I put off resolving this until V2. </p>"},{"location":"blog/2025/04/14/v1-development-and-experimentation/#results","title":"Results","text":"<p>Because the episode trajectory was driven entirely by the competition between the two agents, their behaviors co-evolved over time. This created an emergent curriculum: early on, the agents moved randomly and struggled to reach the ball, but as training progressed, they began to learn to position, chase, and kick. I rendered short videos every few episodes to qualitatively inspect progress.</p>"},{"location":"blog/2025/04/14/v1-development-and-experimentation/#conclusions-and-next-steps","title":"Conclusions and Next Steps","text":"<p>My agents had certainly learned something - they were able to chase the ball around and seemed to have developed basic skills for scoring and defense. However, there are several areas I would like to improve. </p>"},{"location":"blog/2025/04/14/v1-development-and-experimentation/#policy-network-improvements","title":"Policy Network Improvements","text":"<ul> <li>The model is quite small and is likely reaching some limits in its ability to learn more advanced tactics. Scaling up is an easy next step. </li> <li>The model has a very basic architecture, and has almost no special handling for team A vs team B. Adding resonant layers and other architectural tricks will help it learn faster. </li> </ul>"},{"location":"blog/2025/04/14/v1-development-and-experimentation/#training-algorithm-improvements","title":"Training Algorithm Improvements","text":"<ul> <li>The simplicity of the training loop (no critic, no replay buffer, no batch rollout) helped me iterate quickly and focus on the dynamics of multi-agent self-play. But it also exposed some limitations, especially around sample efficiency and variance in learning. Later versions would introduce improvements here.</li> </ul>"},{"location":"blog/2025/04/14/v1-development-and-experimentation/#end-to-end-tensorization","title":"End-to-end Tensorization","text":"<ul> <li>As mentioned above, data is being transfered between torch tensors and native python data types. If I fully tensorize the environment and game physics simulators, I can run the entire training loop in torch, speeding things up dramatically. Note: This is a daunting task and would typically require some serious coffee drinking and squinting at the screen for long periods of time, but it also seems like something AI would be good at!</li> </ul>"},{"location":"blog/2025/04/15/v2-development-and-experimentation/","title":"V2 Development and Experimentation","text":"<p>Recall that at the end of V1, I had concluded with three areas for improvement:</p> <ul> <li>Policy network improvements</li> <li>Training algorithm improvements</li> <li>End-to-end tensorization </li> </ul>"},{"location":"blog/2025/04/15/v2-development-and-experimentation/#end-to-end-tensorization","title":"End-to-end Tensorization","text":"<p>The training loop of V1 wasn't terribly slow, but I knew that scaling it up in its current form with either a more powerful policy network or training algorithm would put a significant drag on development. End-to-end tensorization, i.e., the expression of all simulation environment and training algorithms as tensors, would solve this with a major speedup. I decided to start with this with the hope of unlocking a step change in training speed. </p> <p>Note</p> <p>This is something that I wouldnt normally take on in a side project. Tensorization isn't rocket science, but it requires a sustained level of moderately high mental effort -- it would be like signing up for 3 hours of voluntary math homework. </p> <p>This is a case where having AI code assist absolutely helped me achieve something that I wouldn't have taken on at all. All the cutting edge AI models were more than capable of converting the environment, and I was freed up to operate as a tech lead: thinking of useful utilities that would help build an effective testing harness and visualization compatibility. </p> <p>In V1 I had implemented a batch training script, where multiple games were played at the same time, and update gradients were averaged across the games. Its trivial to generate a batch of predictions from a Policy Network, but the simulation environment was \"vectorized\" by simply running multiple times serially. In V2, we wanted our environment to manage the state of a batch of games by holding all the information in multi-dimensional arrays (in our case, torch tensors). </p> <p>For example, in V1, this is how I handled kicks (the logic is that the ball is kicked by any player within a certain distance of the ball):</p> <pre><code># Sum up all kick forces from players within kicking distance\ntotal_kx = sum(\n    a.kx for p, a in zip(self.team_a, team_a_actions)\n    if self._distance_to_ball(p.position) &lt;= MIN_KICKING_DISTANCE\n) + sum(\n    a.kx for p, a in zip(self.team_b, team_b_actions)\n    if self._distance_to_ball(p.position) &lt;= MIN_KICKING_DISTANCE\n)\n\ntotal_ky = sum(\n    a.ky for p, a in zip(self.team_a, team_a_actions)\n    if self._distance_to_ball(p.position) &lt;= MIN_KICKING_DISTANCE\n) + sum(\n    a.ky for p, a in zip(self.team_b, team_b_actions)\n    if self._distance_to_ball(p.position) &lt;= MIN_KICKING_DISTANCE\n)\n</code></pre> <p>In V2, this became: </p> <pre><code># Calculate distances from players to ball\nball_expanded = self.ball_position.unsqueeze(1)  # [batch_size, 1, 2]\nteam1_to_ball = torch.norm(self.team1_positions - ball_expanded, dim=2)\nteam2_to_ball = torch.norm(self.team2_positions - ball_expanded, dim=2)\n\n# Determine players within kicking distance\nteam1_can_kick = team1_to_ball &lt; MIN_KICKING_DISTANCE\nteam2_can_kick = team2_to_ball &lt; MIN_KICKING_DISTANCE\n\n# Calculate new ball velocity based on kicks\nteam1_kick_mask = team1_can_kick.float().unsqueeze(-1)\nteam1_total_kicks = torch.sum(team1_kick_mask * team1_kick_vel, dim=1)\nteam1_kickers_count = torch.sum(team1_kick_mask, dim=(1, 2))\n\nteam2_kick_mask = team2_can_kick.float().unsqueeze(-1)\nteam2_total_kicks = torch.sum(team2_kick_mask * team2_kick_vel, dim=1)\nteam2_kickers_count = torch.sum(team2_kick_mask, dim=(1, 2))\n\n# Combine team 1 and 2\ntotal_kicks = team1_total_kicks + team2_total_kicks\ntotal_kickers = team1_kickers_count + team2_kickers_count\n\n# Safely average\nkicking_mask = (total_kickers &gt; 0).float().unsqueeze(-1)\nsafe_kickers = torch.clamp(total_kickers.unsqueeze(-1), min=1.0)\naveraged_kicks = total_kicks / safe_kickers\n</code></pre> <p>This is the un-sexy plumbing work of machine learning, but it resulted in significantly faster operations. Unfortunately, I still ran into trouble when trying to run this on a GPU. </p>"},{"location":"blog/2025/04/15/v2-development-and-experimentation/#results","title":"Results","text":"Environment Device Batch Size Iterations per Second Total Games per Second V1 CPU 1 8 8 V1 CPU 64 0.5 32 V2 CPU 64 6 384 V2 GPU 64 0.5 32 V2 GPU 512 0.5 256"},{"location":"example_videos/","title":"Example Videos","text":""},{"location":"example_videos/#typical-start-state","title":"Typical Start State","text":"<p>When the model is initialized, the players move around randomly. In many cases, they will simply drift into the corners as the game state has almost no change. </p>"},{"location":"example_videos/#full-training-run-with-poor-reward-function","title":"Full training run with poor reward function","text":"<p>The players slowly advance to the ball and kick it gently toward the goal. I think this behavior stems from the reward function that I defined for this run. Players are rewarded if their distance to the ball is decreased and if they kick the ball. This encourages them to move directly to the ball and discourages them from kicking the ball hard, which would increase the distance to the ball and make it less likely for them to kick it again. </p>"},{"location":"blog/archive/2025/","title":"2025","text":""}]}